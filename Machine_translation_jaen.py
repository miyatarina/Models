# -*- coding: utf-8 -*-
"""日英機械翻訳.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13CQAlNyFpBimUH5E9xoFdhd0NioxeUiu

# 事前にやること
1. ランタイムのタイプを "GPU" に変更
2. 設定ファイル "rnn.yaml" および "san.yaml" をアップロード
"""

from google.colab import drive
drive.mount('/content/drive')

# 単語分割器のインストール

! pip install mosestokenizer

# ライブラリのインストール

! pip install janome
! pip install OpenNMT-py

# 前処理（日本語の単語分割）

from janome.tokenizer import Tokenizer
tokenizer = Tokenizer()

# train
fout = open("ja.train.txt", "w")
fin = open("/content/drive/MyDrive/Vdata/transrate/ja.comp.train.txt", "r")
for line in fin:
    fout.write(" ".join([token.surface for token in tokenizer.tokenize(line.strip())]) + "\n")
fin.close()
fout.close()
! cp "/content/drive/MyDrive/Vdata/transrate/en.train.txt" "en.train.txt"

# valid
fout = open("ja.valid.txt", "w")
fin = open("/content/drive/MyDrive/Vdata/transrate/ja.comp.valid.txt", "r")
for line in fin:
    fout.write(" ".join([token.surface for token in tokenizer.tokenize(line.strip())]) + "\n")
fin.close()
fout.close()
! cp "/content/drive/MyDrive/Vdata/transrate/en.valid.txt" "en.valid.txt"

# eval
fout = open("ja.eval.txt", "w")
fin = open("/content/drive/MyDrive/Vdata/transrate/ja.comp.eval.txt", "r")
for line in fin:
    fout.write(" ".join([token.surface for token in tokenizer.tokenize(line.strip())]) + "\n")
fin.close()
fout.close()

# データセットの準備

! onmt_build_vocab -config "rnn.yaml" -n_sample 50000

# 訓練

! onmt_train -config "rnn.yaml"

# データセットの準備2

! onmt_build_vocab -config "rnn2.yaml" -n_sample 50000 -overwrite

# 訓練

! onmt_train -config "rnn2.yaml"

# 翻訳

! onmt_translate -model "rnn_step_60000.pt" "2-rnn_step_70000.pt" -src "ja.eval.txt" -output "miyata_pred.rnn.en.txt" -gpu "0" -verbose

# 機械翻訳

from mosestokenizer import MosesTokenizer
tokenizer = MosesTokenizer("en", "no-escape")

fname = "miyata_pred.rnn.en.txt"

fout = open(fname.replace(".txt", ".tok.txt"), "w")
fin = open(fname, "r")
for line in fin:
    fout.write(" ".join([word for word in tokenizer(line.strip().lower())]) + "\n")
fin.close()
fout.close()