# -*- coding: utf-8 -*-
"""感情極性分類.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BMm3iREgCjenyoKPgaxxzEnc5dXT8MvC
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from collections import defaultdict
from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from mlxtend.plotting import plot_decision_regions
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import torch
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import classification_report

f = open("/content/drive/MyDrive/data/label.train.txt", "r")
ltrain = f.read().splitlines()
f = open("/content/drive/MyDrive/data/text.train.txt", "r")
ttrain = f.read().splitlines()
f = open("/content/drive/MyDrive/data/label.test.txt", "r")
ltest = f.read().splitlines()
f = open("/content/drive/MyDrive/data/text.test.txt", "r")
ttest = f.read().splitlines()
f = open("/content/drive/MyDrive/data/label.dev.txt", "r")
ldev = f.read().splitlines()
f = open("/content/drive/MyDrive/data/text.dev.txt", "r")
tdev = f.read().splitlines()
f = open("/content/drive/MyDrive/data/text.eval.txt", "r")
teval = f.read().splitlines()
f.close()

#ttrain_list = list() #ttrainから改行文字を削除したリスト
ttrain_sent = list() #文章ごとに単語に分けたリスト
# ttrain_wordlist = list() 
# ttrain_beforewordlist = list()
ttrain_word = list()
ttrain_countlist = list()
ttest_sent = list()
ttest_countlist = list()
tdev_sent = list()
tdev_countlist = list()
teval_sent = list()
teval_countlist = list()

print(ttrain)

#文章ごとにリストに分けたものを入れる関数
def split_list(t, tsent):
    for sentence in t:
        tsent.append(sentence.split())

split_list(ttrain, ttrain_sent)
split_list(ttest, ttest_sent)
split_list(tdev, tdev_sent)
split_list(teval, teval_sent)
print(ttrain_sent)

#単語一覧を作る
for sentence in ttrain_sent:
    for word in sentence:
        ttrain_word.append(word)

ttrain_word = set(ttrain_word)

# print(ttrain_beforewordlist)
# print(len(ttrain_beforewordlist))

# # あまり意味のない単語をttrain_wordlistから除去
# remove = ['て', 'に', 'を', 'は', 'が', 'と', 'へ', 'よも', 'ケータイ', 'ただただ', 'トップガン', '⊂(', 'コパアメリカ', '珈琲', '探さ', '３', 'おもう', 'かしげ', '口紅', 'おまけ', 'インド', 'gmail', '(__', '今', '丸亀', 'うう', 'うっ', '←', 'ゴンザレス', 'てよ', 'セルフ', 'これから', '事務所', 'ガ', 'ゃ', 'だし', 'まき', 'シフト', '~', 'ミ', 'Ｘ', '青森', '張本', '昨日', '鳴子', '年', 'かめ', '下見', '蘭', 'チン', 'そのうち', '以降', '乳児', 'ブラウン', '東北', '93', '服', 'ビデオ', '外', 'てか', '会場', 'はね', 'よお', '円', 'bot', '絵', 'ビタミン', 'こん', '来週', '昨日', '今週', '今月', '来月', '先月', '今年', '来年', '去年', '昨年', '一昨年', '一昨日']
# ttrain_wordlist = list(set(ttrain_beforewordlist) - set(remove))
# print(len(ttrain_wordlist))

# # ttrain_wordをttrainにて一定回数以上登場する単語のみに絞る
# d = defaultdict(int)

# for word_all in ttrain_wordlist:
#     d[word_all] = 0

# for word_all in ttrain_wordlist:
#     for sentence in ttrain_sent:
#         for word in sentence:
#             if word_all == word:
#                 d[word_all] += 1

# for key, value in d.items():
#     if d[key] >= 3:
#         ttrain_word.append(key)
#         print(key, value)

print(len(ttrain_word))

#各行に単語が出現したかどうかを0か1で入れていく
#nは単語が全く出現しなかった場合の表現、0ならば現れなかったことになる
def appearance(sent_list, countlist):
    for sentence in sent_list:
        count = list()
        for word_all in ttrain_word:
            for word in sentence:
                n = 0
                if word == word_all:
                    count.append(1)
                    n = 1
                    break
            if n == 0:
                count.append(0)
        countlist.append(count)

appearance(ttrain_sent, ttrain_countlist)
appearance(ttest_sent, ttest_countlist)
appearance(tdev_sent, tdev_countlist)
appearance(teval_sent, teval_countlist)

x_train = ttrain_countlist
x_test = ttest_countlist
x_dev = tdev_countlist
x_eval = teval_countlist

#t_trainなどをintに変換
#学習時に負の値が使えないため，負の値を別の数字で置換する
t_train = list()
t_test = list()
t_dev = list()

def replacement(before, after):
    for label in before:
        if (label == '-1'):
            after.append(3)
        elif (label == '-2'):
            after.append(4)
        else:
            after.append(int(label))
    return after

t_train = replacement(ltrain, t_train)
t_test = replacement(ltest, t_test)
t_dev = replacement(ldev, t_dev)

print("変換前：", type(x_train), type(t_train))

x_train = torch.tensor(x_train, dtype=torch.float32)
x_val = torch.tensor(x_dev, dtype=torch.float32)
x_test = torch.tensor(x_test, dtype=torch.float32)
x_eval = torch.tensor(x_eval, dtype=torch.float32)

# x_evalを学習機に適用できるようにするためにすべて0にした3000行のラベルを用意
t_eval = list()
for i in range(0, 3000):
    t_eval.append(0)

t_train = torch.tensor(t_train, dtype=torch.int64)
t_val = torch.tensor(t_dev, dtype=torch.int64)
t_test = torch.tensor(t_test, dtype=torch.int64)
t_eval = torch.tensor(t_eval, dtype=torch.int64)

print("変換後：", type(x_train), type(t_train))

train = TensorDataset(x_train, t_train)
val = TensorDataset(x_val, t_val)
test = TensorDataset(x_test, t_test)
eval = TensorDataset(x_eval, t_eval)

batch_size = 10

train_loader = DataLoader(train, batch_size, shuffle=True)
val_loader = DataLoader(val, batch_size, shuffle=False)
test_loader = DataLoader(test, batch_size, shuffle=False)
eval_loader = DataLoader(eval, batch_size, shuffle=False)

class Net(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(len(ttrain_word), 1000) 
        self.fc2 = nn.Linear(1000, 500) 
        self.fc3 = nn.Linear(500, 5) 
    
    def forward(self, x):
        h = F.relu(self.fc1(x)) 
        h2 = self.fc2(h) 
        y = self.fc3(h2)  
        return y

net = Net()
net

criterion = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

net = net.to(device)

device

max_epoch = 13

for epoch in range(max_epoch):

    for batch in train_loader:

        x, t = batch 

        x = x.to(device)
        t = t.to(device)

        optimizer.zero_grad()

        y = net(x)
        loss = criterion(y, t)

        loss.backward()
        optimizer.step()
    
    with torch.no_grad():
        losses = list()
        for batch in val_loader:
            x, t = batch
            x = x.to(device)
            t = t.to(device)
            y = net(x) 
            loss = criterion(y, t)
            losses.append(loss)
    val_loss = torch.tensor(losses).mean()
    print("Epoch: %02d  val_loss: %.3f" % (epoch+1, val_loss))

with torch.no_grad():
    preds = list()
    for batch in test_loader:
        x, t = batch
        x = x.to(device)
        t = t.to(device)
        y = net(x)  
        preds.append(y.argmax(axis=1)) 
    preds = torch.concat(preds)

preds

golds = torch.concat([t for x, t in test_loader])
print(classification_report(golds, preds, digits=3))

with torch.no_grad():
    preds2 = list()
    for batch in eval_loader:
        x, t = batch
        x = x.to(device)
        t = t.to(device)
        y = net(x)  
        preds2.append(y.argmax(axis=1)) 
    preds2 = torch.concat(preds2)

preds2

y_eval = list()
def replacement2(before, after):
    for label in before:
        if (label == 3):
            after.append('-1')
        elif (label == 4):
            after.append('-2')
        else:
            after.append(str(int(label)))
    return after

replacement2(preds2, y_eval)
print(y_eval)

print(teval[0])
print(y_eval[0])
print(teval[1])
print(y_eval[1])
print(teval[2])
print(y_eval[2])

f = open('y_eval6.txt', 'w')
for i in y_eval:
    f.write(i)
    f.write('\n')

f.close()