# -*- coding: utf-8 -*-
"""英文類似度推定.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17GTrJvrhVoaZEIz3yUaRg8aD_AyFH5Zw
"""

# ライブラリのインストール
! pip install mosestokenizer
! pip install gensim

from google.colab import drive
drive.mount('/content/drive')

etest = list()
with open("/content/drive/MyDrive/Vdata/English/test.text.txt", "r") as f:
    for data in f:
        etest.append(data.rstrip('\n'))

# 単語ベクトルの読み込み1
import gensim
model_path = "/content/drive/MyDrive/Vdata/crawl-300d-2M.vec"
w2v_1 = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)

# 単語ベクトルの読み込み2
import gensim.downloader as api
w2v_2 = api.load("word2vec-google-news-300")

# 前処理: 文 -> 単語リスト
from mosestokenizer import MosesTokenizer
tokenizer = MosesTokenizer("en", "no-escape")

s1 = list()
s2 = list()
tokenize1 = list()
tokenize2 = list()
stoplist1 = list()
stoplist2 = list()
wordslist1 = list()
wordslist2 = list()

# タブで分けてそれぞれs1とs2に入れる
textnum = 0
for sentence in etest:
    textnum += 1
    s = sentence.split("\t")
    s1.append(s[0])
    s2.append(s[1])

# 単語分割
def tokenize(s, tokenize):
    for sentence in s:
        tokenize.append(tokenizer(sentence.lower()))

# 単語を抜く
def stopwords(inlist, outlist):
    stop_words = ["a", "is", ".", ",", "s", "and", "by", "the", "are", "with", "on", "of", "at", "in", "up", "one", "two", "three", "for", "as", "out", "from", "very", "off", "to", "it", "t", ":", "ll", "will", "d", "do", "ve", "m", "she", "he", "her", "you", "we", "us", "your", "my", "me", "yours", "his", "him", "our", "'", ";", "'s", "i", '"']
    for sentence in inlist:
        outlist_stop = list()
        for word in sentence:
            if word not in stop_words:
                outlist_stop.append(word)
        outlist.append(outlist_stop)

# 数字置き換え
def replacenum(inlist, outlist):
    for sentence in inlist:
        replace = list()
        for word in sentence:
            if word.isdigit():
                replace.append("0")
            else:
                replace.append(word)
        outlist.append(replace)    

tokenize(s1, tokenize1)
tokenize(s2, tokenize2)

stopwords(tokenize1, wordslist1)
stopwords(tokenize2, wordslist2)

replacenum(stoplist1, wordslist1)
replacenum(stoplist2, wordslist2)

print(wordslist1)
print(wordslist2)

# wikiからの単語リストの作成
from collections import defaultdict

def wiki_tokenizer(filename):
    i = 0
    word2freq = defaultdict(int) # 出現回数を格納
    word2docnum = defaultdict(int) # 何文書に登場したかを格納
    with open(filename, "r") as f:
        for sentence in f:
            if i == 1000000:
                break
            s = tokenizer(sentence.lower())
            for token in s:
                word2freq[token] += 1
            for token in set(s):
                word2docnum[token] += 1
            i += 1
    return word2freq, word2docnum

word2freq, word2docnum = wiki_tokenizer("/content/drive/MyDrive/Vdata/English/en_wiki.sent.10m.txt")

# 確率の計算
def cal_p(word2freq):
    word2prob = defaultdict(float)
    total = sum(word2freq.values())
    for word in word2freq:
        word2prob[word] = word2freq[word] / total
    return word2prob

word2prob = cal_p(word2freq)

# ベクトルの重み付き平均の算出
import numpy as np

def avg_embedding(word2vec, word2prob):
    vector = np.zeros(word2vec.vector_size)
    for word in word2vec.vocab:
        if word in word2prob:
            vector += word2prob[word] * word2vec[word]
    return vector
    
avg_w2v_1 = avg_embedding(w2v_1, word2prob)
avg_w2v_2 = avg_embedding(w2v_2, word2prob)

# w2v_1 中心化
import numpy as np

vector1_1 = list()
vector2_1 = list()

def vectorize(words, w2v):
    vectors = list()
    for word in words:
        if word in w2v:
            vectors.append(w2v[word] - avg_w2v_1)
    return np.array(vectors).mean(axis=0)

for i in range(textnum):
    vector1_1.append(vectorize(wordslist1[i], w2v_1))
    vector2_1.append(vectorize(wordslist2[i], w2v_1))

# w2v_2 中心化
import numpy as np

vector1_2 = list()
vector2_2 = list()

def vectorize(words, w2v):
    vectors = list()
    for word in words:
        if word in w2v:
            vectors.append(w2v[word] - avg_w2v_2)
    return np.array(vectors).mean(axis=0)

for i in range(textnum):
    vector1_2.append(vectorize(wordslist1[i], w2v_2))
    vector2_2.append(vectorize(wordslist2[i], w2v_2))

# BoW
vector1_bow = list()
vector2_bow = list()

# 語彙（扱いたい全ての単語）
vocab = set()
for (words1, words2) in zip(wordslist1, wordslist2):
    for (word1, word2) in zip(words1, words2):
        vocab.add(word1)
        vocab.add(word2)

# 各単語にIDを割り当てる
word2id = dict()
for word in vocab:
    word2id[word] = len(word2id)

    def vectorize(words, vocab, word2id):
        vector = np.zeros(len(vocab))
        for word in words:
            if word in vocab:
                vector[word2id[word]] = 1
        return vector

for i in range(textnum):
    vector1_bow.append(vectorize(wordslist1[i], vocab, word2id))
    vector2_bow.append(vectorize(wordslist2[i], vocab, word2id))

# 類似度推定: 2つのベクトル -> コサイン類似度
import math

def cos(v1, v2):
    if math.isnan(np.linalg.norm(vector1_1[i]) * np.linalg.norm(vector2_1[i])):
        c = 1
    else:
        c = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
    return c

f = open("Engw2v.txt", "w", encoding="UTF-8")

for i in range(textnum):
    ensamble = (0.4 * cos(vector1_1[i], vector2_1[i]) + 0.3 * cos(vector1_2[i], vector2_2[i]) + 0.3 * cos(vector1_bow[i], vector2_bow[i]))
    print("%1.3f\t%s\t%s" % (ensamble, wordslist1[i], wordslist2[i]))
    f.write(str(ensamble)+"\n")

f.close()